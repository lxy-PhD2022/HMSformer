Args in experiment:
Namespace(accumulation_steps=2, activation='gelu', affine=0, batch_size=128, c_out=7, checkpoints='./checkpoints/', compress_len=10, d_ff=128, d_layers=1, d_model=16, data='ETTh1', data_path='ETTh1.csv', dec_in=7, decomposition=0, des='Exp', device_ids=[0, 1, 2], devices='0,1,2', distil=True, do_predict=False, dropout=0.3, dvices='0,1,2', e_layers=3, embed='timeF', embed_type=0, enc_in=7, factor=1, fc_dropout=0.3, features='M', freq='h', gpu=0, head_dropout=0.0, individual=0, is_training=1, itr=1, kernel_size=25, label_len=18, learning_rate=0.0001, loss='mse', lradj='type3', model='MGSformer_TST', model_id='96_720', momentum=0.8, moving_avg=25, n=2, n_heads=4, n_intra=2, num_workers=10, output_attention=False, padding_patch='end', patch_len=16, patience=8, pct_start=0.3, period=24, pred_len=720, random_seed=2021, revin=1, root_path='./dataset/', seq_len=96, stride=8, subtract_last=0, target='OT', test_flop=False, traffic=0, train_epochs=100, use_amp=False, use_gpu=True, use_multi_gpu=True, x=5)
Use GPU: cuda:0
>>>>>>>start training : 24_2_2_96_720_MGSformer_TST_ETTh1_ftM_sl96_ll18_pl720_dm20_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
module.model.backbone.W_pos_intra: 80
module.model.backbone.W_pos: 480
module.model.backbone.W_pos_coarse: 140
module.model.backbone.W_P_intra.weight: 960
module.model.backbone.W_P_intra.bias: 20
module.model.backbone.W_P.weight: 240
module.model.backbone.W_P.bias: 20
module.model.backbone.W_P_coarse.weight: 1400
module.model.backbone.W_P_coarse.bias: 20
module.model.backbone.encoder.layers.0.self_attn.W_Q.weight: 400
module.model.backbone.encoder.layers.0.self_attn.W_Q.bias: 20
module.model.backbone.encoder.layers.0.self_attn.W_K.weight: 400
module.model.backbone.encoder.layers.0.self_attn.W_K.bias: 20
module.model.backbone.encoder.layers.0.self_attn.W_V.weight: 400
module.model.backbone.encoder.layers.0.self_attn.W_V.bias: 20
module.model.backbone.encoder.layers.0.self_attn.to_out.0.weight: 400
module.model.backbone.encoder.layers.0.self_attn.to_out.0.bias: 20
module.model.backbone.encoder.layers.0.norm_attn.1.weight: 20
module.model.backbone.encoder.layers.0.norm_attn.1.bias: 20
module.model.backbone.encoder.layers.0.ff.0.weight: 2560
module.model.backbone.encoder.layers.0.ff.0.bias: 128
module.model.backbone.encoder.layers.0.ff.3.weight: 2560
module.model.backbone.encoder.layers.0.ff.3.bias: 20
module.model.backbone.encoder.layers.0.norm_ffn.1.weight: 20
module.model.backbone.encoder.layers.0.norm_ffn.1.bias: 20
module.model.backbone.encoder.layers.1.self_attn.W_Q.weight: 400
module.model.backbone.encoder.layers.1.self_attn.W_Q.bias: 20
module.model.backbone.encoder.layers.1.self_attn.W_K.weight: 400
module.model.backbone.encoder.layers.1.self_attn.W_K.bias: 20
module.model.backbone.encoder.layers.1.self_attn.W_V.weight: 400
module.model.backbone.encoder.layers.1.self_attn.W_V.bias: 20
module.model.backbone.encoder.layers.1.self_attn.to_out.0.weight: 400
module.model.backbone.encoder.layers.1.self_attn.to_out.0.bias: 20
module.model.backbone.encoder.layers.1.norm_attn.1.weight: 20
module.model.backbone.encoder.layers.1.norm_attn.1.bias: 20
module.model.backbone.encoder.layers.1.ff.0.weight: 2560
module.model.backbone.encoder.layers.1.ff.0.bias: 128
module.model.backbone.encoder.layers.1.ff.3.weight: 2560
module.model.backbone.encoder.layers.1.ff.3.bias: 20
module.model.backbone.encoder.layers.1.norm_ffn.1.weight: 20
module.model.backbone.encoder.layers.1.norm_ffn.1.bias: 20
module.model.backbone.encoder.layers.2.self_attn.W_Q.weight: 400
module.model.backbone.encoder.layers.2.self_attn.W_Q.bias: 20
module.model.backbone.encoder.layers.2.self_attn.W_K.weight: 400
module.model.backbone.encoder.layers.2.self_attn.W_K.bias: 20
module.model.backbone.encoder.layers.2.self_attn.W_V.weight: 400
module.model.backbone.encoder.layers.2.self_attn.W_V.bias: 20
module.model.backbone.encoder.layers.2.self_attn.to_out.0.weight: 400
module.model.backbone.encoder.layers.2.self_attn.to_out.0.bias: 20
module.model.backbone.encoder.layers.2.norm_attn.1.weight: 20
module.model.backbone.encoder.layers.2.norm_attn.1.bias: 20
module.model.backbone.encoder.layers.2.ff.0.weight: 2560
module.model.backbone.encoder.layers.2.ff.0.bias: 128
module.model.backbone.encoder.layers.2.ff.3.weight: 2560
module.model.backbone.encoder.layers.2.ff.3.bias: 20
module.model.backbone.encoder.layers.2.norm_ffn.1.weight: 20
module.model.backbone.encoder.layers.2.norm_ffn.1.bias: 20
module.model.backbone.encoder_intra.layers.0.self_attn.W_Q.weight: 400
module.model.backbone.encoder_intra.layers.0.self_attn.W_Q.bias: 20
module.model.backbone.encoder_intra.layers.0.self_attn.W_K.weight: 400
module.model.backbone.encoder_intra.layers.0.self_attn.W_K.bias: 20
module.model.backbone.encoder_intra.layers.0.self_attn.W_V.weight: 400
module.model.backbone.encoder_intra.layers.0.self_attn.W_V.bias: 20
module.model.backbone.encoder_intra.layers.0.self_attn.to_out.0.weight: 400
module.model.backbone.encoder_intra.layers.0.self_attn.to_out.0.bias: 20
module.model.backbone.encoder_intra.layers.0.norm_attn.1.weight: 20
module.model.backbone.encoder_intra.layers.0.norm_attn.1.bias: 20
module.model.backbone.encoder_intra.layers.0.ff.0.weight: 2560
module.model.backbone.encoder_intra.layers.0.ff.0.bias: 128
module.model.backbone.encoder_intra.layers.0.ff.3.weight: 2560
module.model.backbone.encoder_intra.layers.0.ff.3.bias: 20
module.model.backbone.encoder_intra.layers.0.norm_ffn.1.weight: 20
module.model.backbone.encoder_intra.layers.0.norm_ffn.1.bias: 20
module.model.backbone.encoder_intra.layers.1.self_attn.W_Q.weight: 400
module.model.backbone.encoder_intra.layers.1.self_attn.W_Q.bias: 20
module.model.backbone.encoder_intra.layers.1.self_attn.W_K.weight: 400
module.model.backbone.encoder_intra.layers.1.self_attn.W_K.bias: 20
module.model.backbone.encoder_intra.layers.1.self_attn.W_V.weight: 400
module.model.backbone.encoder_intra.layers.1.self_attn.W_V.bias: 20
module.model.backbone.encoder_intra.layers.1.self_attn.to_out.0.weight: 400
module.model.backbone.encoder_intra.layers.1.self_attn.to_out.0.bias: 20
module.model.backbone.encoder_intra.layers.1.norm_attn.1.weight: 20
module.model.backbone.encoder_intra.layers.1.norm_attn.1.bias: 20
module.model.backbone.encoder_intra.layers.1.ff.0.weight: 2560
module.model.backbone.encoder_intra.layers.1.ff.0.bias: 128
module.model.backbone.encoder_intra.layers.1.ff.3.weight: 2560
module.model.backbone.encoder_intra.layers.1.ff.3.bias: 20
module.model.backbone.encoder_intra.layers.1.norm_ffn.1.weight: 20
module.model.backbone.encoder_intra.layers.1.norm_ffn.1.bias: 20
module.model.backbone.encoder_intra.layers.2.self_attn.W_Q.weight: 400
module.model.backbone.encoder_intra.layers.2.self_attn.W_Q.bias: 20
module.model.backbone.encoder_intra.layers.2.self_attn.W_K.weight: 400
module.model.backbone.encoder_intra.layers.2.self_attn.W_K.bias: 20
module.model.backbone.encoder_intra.layers.2.self_attn.W_V.weight: 400
module.model.backbone.encoder_intra.layers.2.self_attn.W_V.bias: 20
module.model.backbone.encoder_intra.layers.2.self_attn.to_out.0.weight: 400
module.model.backbone.encoder_intra.layers.2.self_attn.to_out.0.bias: 20
module.model.backbone.encoder_intra.layers.2.norm_attn.1.weight: 20
module.model.backbone.encoder_intra.layers.2.norm_attn.1.bias: 20
module.model.backbone.encoder_intra.layers.2.ff.0.weight: 2560
module.model.backbone.encoder_intra.layers.2.ff.0.bias: 128
module.model.backbone.encoder_intra.layers.2.ff.3.weight: 2560
module.model.backbone.encoder_intra.layers.2.ff.3.bias: 20
module.model.backbone.encoder_intra.layers.2.norm_ffn.1.weight: 20
module.model.backbone.encoder_intra.layers.2.norm_ffn.1.bias: 20
module.model.backbone.encoder_coarse.layers.0.self_attn.W_Q.weight: 400
module.model.backbone.encoder_coarse.layers.0.self_attn.W_Q.bias: 20
module.model.backbone.encoder_coarse.layers.0.self_attn.W_K.weight: 400
module.model.backbone.encoder_coarse.layers.0.self_attn.W_K.bias: 20
module.model.backbone.encoder_coarse.layers.0.self_attn.W_V.weight: 400
module.model.backbone.encoder_coarse.layers.0.self_attn.W_V.bias: 20
module.model.backbone.encoder_coarse.layers.0.self_attn.to_out.0.weight: 400
module.model.backbone.encoder_coarse.layers.0.self_attn.to_out.0.bias: 20
module.model.backbone.encoder_coarse.layers.0.norm_attn.1.weight: 20
module.model.backbone.encoder_coarse.layers.0.norm_attn.1.bias: 20
module.model.backbone.encoder_coarse.layers.0.ff.0.weight: 2560
module.model.backbone.encoder_coarse.layers.0.ff.0.bias: 128
module.model.backbone.encoder_coarse.layers.0.ff.3.weight: 2560
module.model.backbone.encoder_coarse.layers.0.ff.3.bias: 20
module.model.backbone.encoder_coarse.layers.0.norm_ffn.1.weight: 20
module.model.backbone.encoder_coarse.layers.0.norm_ffn.1.bias: 20
module.model.backbone.encoder_coarse.layers.1.self_attn.W_Q.weight: 400
module.model.backbone.encoder_coarse.layers.1.self_attn.W_Q.bias: 20
module.model.backbone.encoder_coarse.layers.1.self_attn.W_K.weight: 400
module.model.backbone.encoder_coarse.layers.1.self_attn.W_K.bias: 20
module.model.backbone.encoder_coarse.layers.1.self_attn.W_V.weight: 400
module.model.backbone.encoder_coarse.layers.1.self_attn.W_V.bias: 20
module.model.backbone.encoder_coarse.layers.1.self_attn.to_out.0.weight: 400
module.model.backbone.encoder_coarse.layers.1.self_attn.to_out.0.bias: 20
module.model.backbone.encoder_coarse.layers.1.norm_attn.1.weight: 20
module.model.backbone.encoder_coarse.layers.1.norm_attn.1.bias: 20
module.model.backbone.encoder_coarse.layers.1.ff.0.weight: 2560
module.model.backbone.encoder_coarse.layers.1.ff.0.bias: 128
module.model.backbone.encoder_coarse.layers.1.ff.3.weight: 2560
module.model.backbone.encoder_coarse.layers.1.ff.3.bias: 20
module.model.backbone.encoder_coarse.layers.1.norm_ffn.1.weight: 20
module.model.backbone.encoder_coarse.layers.1.norm_ffn.1.bias: 20
module.model.backbone.encoder_coarse.layers.2.self_attn.W_Q.weight: 400
module.model.backbone.encoder_coarse.layers.2.self_attn.W_Q.bias: 20
module.model.backbone.encoder_coarse.layers.2.self_attn.W_K.weight: 400
module.model.backbone.encoder_coarse.layers.2.self_attn.W_K.bias: 20
module.model.backbone.encoder_coarse.layers.2.self_attn.W_V.weight: 400
module.model.backbone.encoder_coarse.layers.2.self_attn.W_V.bias: 20
module.model.backbone.encoder_coarse.layers.2.self_attn.to_out.0.weight: 400
module.model.backbone.encoder_coarse.layers.2.self_attn.to_out.0.bias: 20
module.model.backbone.encoder_coarse.layers.2.norm_attn.1.weight: 20
module.model.backbone.encoder_coarse.layers.2.norm_attn.1.bias: 20
module.model.backbone.encoder_coarse.layers.2.ff.0.weight: 2560
module.model.backbone.encoder_coarse.layers.2.ff.0.bias: 128
module.model.backbone.encoder_coarse.layers.2.ff.3.weight: 2560
module.model.backbone.encoder_coarse.layers.2.ff.3.bias: 20
module.model.backbone.encoder_coarse.layers.2.norm_ffn.1.weight: 20
module.model.backbone.encoder_coarse.layers.2.norm_ffn.1.bias: 20
module.model.head.linear.weight: 1454400
module.model.head.linear.bias: 720
module.model.compress.weight: 960
module.model.compress.bias: 10
Total trainable parameters: 1522702
train 7825
val 2161
test 2161
Epoch: 1 cost time: 10.186220645904541
[2024-12-07 20:26:55] [32mIntermediate result: 0.733545  (Index 0)[0m
Epoch: 1, Steps: 61 | Train Loss: 0.8673992 Vali Loss: 1.9489019 Test Loss: 0.7335450
Validation loss decreased (inf --> 1.948902).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 9.372071743011475
[2024-12-07 20:27:07] [32mIntermediate result: 0.49083048  (Index 1)[0m
Epoch: 2, Steps: 61 | Train Loss: 0.7229112 Vali Loss: 1.5937765 Test Loss: 0.4908305
Validation loss decreased (1.948902 --> 1.593776).  Saving model ...
Updating learning rate to 0.0001
Epoch: 3 cost time: 9.762600183486938
[2024-12-07 20:27:21] [32mIntermediate result: 0.48095655  (Index 2)[0m
Epoch: 3, Steps: 61 | Train Loss: 0.6627118 Vali Loss: 1.5710979 Test Loss: 0.4809566
Validation loss decreased (1.593776 --> 1.571098).  Saving model ...
Updating learning rate to 0.0001
Epoch: 4 cost time: 9.365470886230469
[2024-12-07 20:27:33] [32mIntermediate result: 0.48250294  (Index 3)[0m
Epoch: 4, Steps: 61 | Train Loss: 0.6422697 Vali Loss: 1.5729043 Test Loss: 0.4825029
EarlyStopping counter: 1 out of 8
Updating learning rate to 9e-05
Epoch: 5 cost time: 9.397473096847534
[2024-12-07 20:27:46] [32mIntermediate result: 0.4869007  (Index 4)[0m
Epoch: 5, Steps: 61 | Train Loss: 0.6331252 Vali Loss: 1.5786672 Test Loss: 0.4869007
EarlyStopping counter: 2 out of 8
Updating learning rate to 8.1e-05
Epoch: 6 cost time: 9.597368955612183
[2024-12-07 20:27:59] [32mIntermediate result: 0.49003386  (Index 5)[0m
Epoch: 6, Steps: 61 | Train Loss: 0.6276893 Vali Loss: 1.5796554 Test Loss: 0.4900339
EarlyStopping counter: 3 out of 8
Updating learning rate to 7.290000000000001e-05
Epoch: 7 cost time: 9.36995005607605
[2024-12-07 20:28:11] [32mIntermediate result: 0.49099708  (Index 6)[0m
Epoch: 7, Steps: 61 | Train Loss: 0.6243725 Vali Loss: 1.5772556 Test Loss: 0.4909971
EarlyStopping counter: 4 out of 8
Updating learning rate to 6.561e-05
Epoch: 8 cost time: 9.307746648788452
[2024-12-07 20:28:24] [32mIntermediate result: 0.4895193  (Index 7)[0m
Epoch: 8, Steps: 61 | Train Loss: 0.6209548 Vali Loss: 1.5738784 Test Loss: 0.4895193
EarlyStopping counter: 5 out of 8
Updating learning rate to 5.904900000000001e-05
Epoch: 9 cost time: 9.640964269638062
[2024-12-07 20:28:36] [32mIntermediate result: 0.49233  (Index 8)[0m
Epoch: 9, Steps: 61 | Train Loss: 0.6192605 Vali Loss: 1.5724556 Test Loss: 0.4923300
EarlyStopping counter: 6 out of 8
Updating learning rate to 5.3144100000000005e-05
Epoch: 10 cost time: 9.376227855682373
[2024-12-07 20:28:49] [32mIntermediate result: 0.4910814  (Index 9)[0m
Epoch: 10, Steps: 61 | Train Loss: 0.6174493 Vali Loss: 1.5666744 Test Loss: 0.4910814
Validation loss decreased (1.571098 --> 1.566674).  Saving model ...
Updating learning rate to 4.782969000000001e-05
Epoch: 11 cost time: 9.15180492401123
[2024-12-07 20:29:01] [32mIntermediate result: 0.49220425  (Index 10)[0m
Epoch: 11, Steps: 61 | Train Loss: 0.6163465 Vali Loss: 1.5757666 Test Loss: 0.4922042
EarlyStopping counter: 1 out of 8
Updating learning rate to 4.304672100000001e-05
Epoch: 12 cost time: 9.42465591430664
[2024-12-07 20:29:13] [32mIntermediate result: 0.49093664  (Index 11)[0m
Epoch: 12, Steps: 61 | Train Loss: 0.6144303 Vali Loss: 1.5725708 Test Loss: 0.4909366
EarlyStopping counter: 2 out of 8
Updating learning rate to 3.874204890000001e-05
Epoch: 13 cost time: 9.370268106460571
[2024-12-07 20:29:26] [32mIntermediate result: 0.4915771  (Index 12)[0m
Epoch: 13, Steps: 61 | Train Loss: 0.6135283 Vali Loss: 1.5741146 Test Loss: 0.4915771
EarlyStopping counter: 3 out of 8
Updating learning rate to 3.486784401000001e-05
Epoch: 14 cost time: 9.477770328521729
[2024-12-07 20:29:39] [32mIntermediate result: 0.4918984  (Index 13)[0m
Epoch: 14, Steps: 61 | Train Loss: 0.6123864 Vali Loss: 1.5764790 Test Loss: 0.4918984
EarlyStopping counter: 4 out of 8
Updating learning rate to 3.138105960900001e-05
Epoch: 15 cost time: 9.333029985427856
[2024-12-07 20:29:51] [32mIntermediate result: 0.49281532  (Index 14)[0m
Epoch: 15, Steps: 61 | Train Loss: 0.6119074 Vali Loss: 1.5715386 Test Loss: 0.4928153
EarlyStopping counter: 5 out of 8
Updating learning rate to 2.824295364810001e-05
Epoch: 16 cost time: 9.745827436447144
[2024-12-07 20:30:04] [32mIntermediate result: 0.49095333  (Index 15)[0m
Epoch: 16, Steps: 61 | Train Loss: 0.6105546 Vali Loss: 1.5715587 Test Loss: 0.4909533
EarlyStopping counter: 6 out of 8
Updating learning rate to 2.541865828329001e-05
Epoch: 17 cost time: 9.340728282928467
[2024-12-07 20:30:17] [32mIntermediate result: 0.4915676  (Index 16)[0m
Epoch: 17, Steps: 61 | Train Loss: 0.6104569 Vali Loss: 1.5719242 Test Loss: 0.4915676
EarlyStopping counter: 7 out of 8
Updating learning rate to 2.287679245496101e-05
Epoch: 18 cost time: 9.322729110717773
[2024-12-07 20:30:29] [32mIntermediate result: 0.4911763  (Index 17)[0m
Epoch: 18, Steps: 61 | Train Loss: 0.6096595 Vali Loss: 1.5723670 Test Loss: 0.4911763
EarlyStopping counter: 8 out of 8
Early stopping
>>>>>>>testing : 24_2_2_96_720_MGSformer_TST_ETTh1_ftM_sl96_ll18_pl720_dm20_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.49108174443244934, mae:0.47305747866630554, rse:0.6730726361274719
[2024-12-07 20:30:32] [32mFinal result: 0.49108174[0m
